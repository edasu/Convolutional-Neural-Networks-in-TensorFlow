# Convolutional-Neural-Networks-in-TensorFlow

<h3>Step 1 | Exploring a Larger Dataset </h3>
<p>In section we'll go deeper into using ConvNets will real-world data, and learn about techniques tha twe can use to improve our ConvNet performance, particularly when doing image classification! Get started by looking at a larger dataset: The Cats and Dogs dataset which had been a Kaggle Challenge in image classification!
<p>
<h3>Step 2 | Augmentation: A technique to avoid overfitting </h3>
<p>You've heard the term overfitting a number of times to this point. Overfitting is simply the concept of being over specialized in training -- namely that your model is very good at classifying what it is trained for, but not so good at classifying things that it hasn't seen. In order to generalize your model more effectively, you will of course need a greater breadth of samples to train it on. That's not always possible, but a nice potential shortcut to this is Image Augmentation, where you tweak the training set to potentially increase the diversity of subjects it covers. You'll learn all about that this week!
 <p>
<h3>Step 3 | Transfer Learning</h3>
<p>Building models for yourself is great, and can be very powerful. But, as you've seen, you can be limited by the data you have on hand. Not everybody has access to massive datasets or the compute power that's needed to train them effectively. Transfer learning can help solve this -- where people with models trained on large datasets train them, so that you can either use them directly, or, you can use the features that they have learned and apply them to your scenario. This is Transfer learning, and you'll look into that this week!
  <p>
<h3>Step 4 | Multiclass Classifications </h3>
<p>When moving beyond binary into Categorical classification there are some coding considerations you need to take into account.
